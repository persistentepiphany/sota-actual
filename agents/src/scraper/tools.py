"""
Scraper Agent Tools

SpoonOS tools for:
- TikTok scraping via Bright Data
- General web scraping
- NeoFS storage
- Job-specific operations

These are the EXECUTION tools - bidding is handled by shared bidding_tools.
"""

import os
import json
import asyncio
from typing import Any, Optional
from datetime import datetime

import httpx
from pydantic import Field
from web3 import Web3

from spoon_ai.tools.base import BaseTool

from ..shared.neofs import get_neofs_client


class TikTokScrapeTool(BaseTool):
    """
    Tool to scrape TikTok content using Bright Data API.
    """
    name: str = "scrape_tiktok"
    description: str = """
    Scrape TikTok videos and posts using Bright Data's Dataset API.
    
    Uses the documented Bright Data dataset scrape endpoint (discover by URL).
    Defaults to dataset_id gd_lu702nij2f790tmv9h unless overridden via
    BRIGHT_DATA_DATASET_ID. Provide a search_query and it will crawl the TikTok
    search page for that query. You can also pass an explicit tiktok_url to
    scrape a specific video or discover page.
    """
    parameters: dict = {
        "type": "object",
        "properties": {
            "search_query": {
                "type": "string",
                "description": "The search query for TikTok (e.g., 'moscow restaurants trendy')"
            },
            "tiktok_url": {
                "type": "string",
                "description": "Optional explicit TikTok URL to scrape (video or discover)"
            },
            "profile_url": {
                "type": "string",
                "description": "TikTok profile URL to scrape posts from"
            },
            "hashtags": {
                "type": "string",
                "description": "Comma-separated hashtags to filter returned videos (e.g. 'food,travel')."
            },
            "max_results": {
                "type": "integer",
                "description": "Maximum number of results to return (default: 10, max: 50)"
            },
            "location": {
                "type": "string",
                "description": "Optional location filter (e.g., 'Russia', 'Moscow')"
            }
        },
        "required": []
    }
    
    async def execute(
        self, 
        search_query: str = "", 
        tiktok_url: str = None,
        profile_url: str = None,
        hashtags: str = "",
        max_results: int = 10,
        location: str = None
    ) -> str:
        """Scrape TikTok using Bright Data dataset scrape endpoint"""
        api_token = os.getenv("BRIGHT_DATA_API_KEY")
        dataset_id = os.getenv("BRIGHT_DATA_DATASET_ID", "gd_lu702nij2f790tmv9h")

        if not api_token:
            return json.dumps(
                {"success": False, "error": "BRIGHT_DATA_API_KEY not configured"}
            )

        if not search_query and not tiktok_url and not profile_url:
            return json.dumps(
                {"success": False, "error": "Provide search_query or tiktok_url or profile_url"}
            )

        max_results = min(max_results, 50)  # Cap at 50

        try:
            # Choose discover mode
            discover_by = "profile_url" if profile_url else "url"

            api_url = (
                "https://api.brightdata.com/datasets/v3/scrape"
                f"?dataset_id={dataset_id}"
                "&notify=false&include_errors=true&type=discover_new"
                f"&discover_by={discover_by}"
                f"&limit_per_input={max_results}"
            )

            # Build input URLs
            inputs = []
            if search_query:
                tiktok_search_url = f"https://www.tiktok.com/search?q={search_query.replace(' ', '+')}"
                inputs.append({"url": tiktok_search_url})
            if tiktok_url:
                inputs.append({"url": tiktok_url})
            if profile_url:
                inputs.append({"url": profile_url})

            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(
                    api_url,
                    headers={
                        "Authorization": f"Bearer {api_token}",
                        "Content-Type": "application/json",
                    },
                    json={"input": inputs},
                )
                response.raise_for_status()

                # Bright Data sometimes replies with JSONL; parse accordingly
                content_type = response.headers.get("content-type", "")
                if "jsonl" in content_type:
                    parsed_lines = self._parse_jsonl(response.text)
                    first = parsed_lines[0] if parsed_lines else {}
                    # If error in first line, return it
                    if first.get("error"):
                        return json.dumps(
                            {
                                "success": False,
                                "error": first.get("error"),
                                "error_code": first.get("error_code"),
                                "dataset_id": dataset_id,
                                "query": search_query,
                                "url": tiktok_url or profile_url or inputs[0].get("url"),
                            },
                            indent=2,
                        )
                    parsed_lines = self._filter_by_hashtags(parsed_lines, hashtags)
                    return json.dumps(
                        {
                            "success": True,
                            "query": search_query,
                            "location": location,
                            "dataset_id": dataset_id,
                            "results": parsed_lines,
                        },
                        indent=2,
                    )

                data = response.json()

                # If Bright Data returns snapshot_id (async), poll until ready
                snapshot_id = data.get("snapshot_id")
                if snapshot_id:
                    ready = await self._poll_snapshot(client, api_token, snapshot_id)
                    ready_results = ready.get("data") or ready.get("result") or ready
                    ready_results = self._filter_by_hashtags(
                        ready_results if isinstance(ready_results, list) else [ready_results],
                        hashtags,
                    )
                    return json.dumps(
                        {
                            "success": True,
                            "query": search_query,
                            "location": location,
                            "dataset_id": dataset_id,
                            "snapshot_id": snapshot_id,
                            "results": ready_results,
                            "status": ready.get("status", "ready"),
                        },
                        indent=2,
                    )

                # If immediate error
                if data.get("error"):
                    return json.dumps(
                        {
                            "success": False,
                            "error": data.get("error"),
                            "error_code": data.get("error_code"),
                            "dataset_id": dataset_id,
                        },
                        indent=2,
                    )

                # Return whatever payload came back
                filtered = self._filter_by_hashtags(
                    data if isinstance(data, list) else [data],
                    hashtags,
                )
                return json.dumps(
                    {
                        "success": True,
                        "query": search_query,
                        "location": location,
                        "dataset_id": dataset_id,
                        "results": filtered,
                    },
                    indent=2,
                )

        except httpx.HTTPError as e:
            return json.dumps({"success": False, "error": f"HTTP error: {str(e)}"})
        except Exception as e:
            return json.dumps({"success": False, "error": str(e)})

    def _parse_jsonl(self, text: str) -> list:
        """Parse JSONL content into a list of dicts."""
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        out = []
        for ln in lines:
            try:
                out.append(json.loads(ln))
            except json.JSONDecodeError:
                out.append({"raw": ln, "parse_error": True})
        return out

    def _filter_by_hashtags(self, results: list, hashtags: str) -> list:
        """Filter results to only those containing any of the specified hashtags."""
        tags = [t.strip().lstrip("#").lower() for t in hashtags.split(",") if t.strip()]
        if not tags:
            return results

        filtered = []
        for item in results:
            item_tags = item.get("hashtags") or []
            normalized = [t.lstrip("#").lower() for t in item_tags if isinstance(t, str)]
            if any(tag in normalized for tag in tags):
                filtered.append(item)
        return filtered

    async def _poll_snapshot(
        self,
        client: httpx.AsyncClient,
        api_token: str,
        snapshot_id: str,
        max_attempts: int = 10,
        delay_seconds: float = 5.0,
    ) -> dict:
        """Poll Bright Data snapshot until ready or timeout."""
        url = f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}"
        for attempt in range(max_attempts):
            resp = await client.get(url, headers={"Authorization": f"Bearer {api_token}"})
            if resp.status_code != 200:
                await asyncio.sleep(delay_seconds)
                continue
            content_type = resp.headers.get("content-type", "")
            if "jsonl" in content_type:
                parsed = self._parse_jsonl(resp.text)
                return {"status": "ready", "data": parsed}
            data = resp.json()
            if data.get("status") == "ready":
                return data
            await asyncio.sleep(delay_seconds)
        return {"status": "timeout", "snapshot_id": snapshot_id}
    
    async def _poll_for_results(
        self, 
        client: httpx.AsyncClient, 
        api_token: str, 
        snapshot_id: str,
        max_attempts: int = 20
    ) -> list:
        """Poll Bright Data for scraping results"""
        base_delay = 3
        
        for attempt in range(max_attempts):
            try:
                response = await client.get(
                    f"https://api.brightdata.com/datasets/v3/snapshot/{snapshot_id}",
                    headers={"Authorization": f"Bearer {api_token}"}
                )
                
                if response.status_code == 200:
                    data = response.json()
                    if data.get("status") == "ready":
                        return data.get("data", [])
                    elif data.get("status") == "failed":
                        raise Exception(f"Scrape failed: {data.get('error')}")
                
                # Exponential backoff
                delay = base_delay * (1.5 ** attempt)
                await asyncio.sleep(min(delay, 60))
                
            except httpx.HTTPError:
                await asyncio.sleep(base_delay)
        
        raise Exception("Scrape timeout - results not ready")


class WebScrapeTool(BaseTool):
    """
    Tool for general web scraping.
    """
    name: str = "web_scrape"
    description: str = """
    Scrape content from web pages.
    
    Fetches and extracts content from a given URL or search query.
    """
    parameters: dict = {
        "type": "object",
        "properties": {
            "url": {
                "type": "string",
                "description": "The URL to scrape"
            },
            "search_query": {
                "type": "string",
                "description": "Alternative: search query (will use search engine)"
            }
        },
        "required": []
    }
    
    async def execute(self, url: str = None, search_query: str = None) -> str:
        """Scrape web content"""
        if not url and not search_query:
            return json.dumps({
                "success": False,
                "error": "Either url or search_query is required"
            })
        
        try:
            async with httpx.AsyncClient(
                timeout=30.0,
                follow_redirects=True
            ) as client:
                target_url = url or f"https://www.google.com/search?q={search_query}"
                
                response = await client.get(
                    target_url,
                    headers={
                        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
                    }
                )
                response.raise_for_status()
                
                # Return raw HTML (in production, parse with BeautifulSoup)
                content = response.text
                
                return json.dumps({
                    "success": True,
                    "url": target_url,
                    "content_length": len(content),
                    "content_preview": content[:1000] if len(content) > 1000 else content
                }, indent=2)
                
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": str(e)
            })


class UploadToNeoFSTool(BaseTool):
    """
    Tool to upload scraping results to NeoFS.
    """
    name: str = "upload_to_neofs"
    description: str = """
    Upload scraping results to NeoFS for decentralized storage.
    
    Returns the object ID which can be used as proof-of-work.
    """
    parameters: dict = {
        "type": "object",
        "properties": {
            "data": {
                "type": "object",
                "description": "The data to upload (JSON serializable)"
            },
            "job_id": {
                "type": "integer",
                "description": "The job ID this result belongs to"
            },
            "source": {
                "type": "string",
                "description": "Source identifier (e.g., 'tiktok', 'web')"
            }
        },
        "required": ["data", "job_id", "source"]
    }
    
    async def execute(self, data: dict, job_id: int, source: str) -> str:
        """Upload data to NeoFS"""
        try:
            neofs = get_neofs_client()
            
            result = await neofs.upload_scraping_results(
                data,
                str(job_id),
                source
            )
            
            await neofs.close()
            
            return json.dumps({
                "success": True,
                "object_id": result.object_id,
                "container_id": result.container_id,
                "job_id": job_id,
                "source": source
            }, indent=2)
            
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": str(e)
            })


class ComputeProofHashTool(BaseTool):
    """
    Tool to compute proof hash from NeoFS object ID.
    """
    name: str = "compute_proof_hash"
    description: str = """
    Compute a proof hash from a NeoFS object ID for delivery submission.
    """
    parameters: dict = {
        "type": "object",
        "properties": {
            "neofs_object_id": {
                "type": "string",
                "description": "The NeoFS object ID"
            }
        },
        "required": ["neofs_object_id"]
    }
    
    async def execute(self, neofs_object_id: str) -> str:
        """Compute proof hash"""
        try:
            proof_hash = Web3.keccak(text=neofs_object_id)
            
            return json.dumps({
                "success": True,
                "neofs_object_id": neofs_object_id,
                "proof_hash": proof_hash.hex()
            }, indent=2)
            
        except Exception as e:
            return json.dumps({
                "success": False,
                "error": str(e)
            })


def create_scraper_tools() -> list[BaseTool]:
    """
    Create all scraper-specific tools.
    
    Note: Bidding and wallet tools are created separately in the agent.
    """
    return [
        TikTokScrapeTool(),
        WebScrapeTool(),
        UploadToNeoFSTool(),
        ComputeProofHashTool(),
    ]

